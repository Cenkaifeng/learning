# 数字类型
ECMAScript 的数字类型使用 IEEE754 标准来表示证书和浮点数值。

IEEE754 标准，IEEE 二进制浮点数算术标准，这个标准定义了表示浮点数的格式等内容。

在 IEEE754 中，规定了四种表示浮点数值的方式：单精确度（32位）、双精确度（64位）、延伸单精确度、与延伸双精确度。像 ECMAScript 采用的就是双精确度，也就是说，会用 64 位字节来储存一个浮点数。

# 浮点数转二进制
1020 用二进制来表示呢？

> 1020 = 1 * 2^9 + 1 * 2^8 + 1 * 2^7 + 1 * 2^6 + 1 * 2^5 + 1 * 2^4 + 1 * 2^3 + 1 * 2^2 + 0 * 2^1 + 0 * 2^0

所以 1020 的二进制为`1111111100`  

而 0.1 用二进制表示就是`0.00011001100110011……`，尾数这个计算不停循环

# 浮点数存储
0.1 转成二进制是一个无限循环的数，但是 ECMAScript 使用 64 位字节来存储一个浮点数。以下是 IEEE754 标准规定的存储方式：
> Value = sign * exponent * fraction

0.1 对应 64 个字节位表示就是：
> 0 01111111011 1001100110011001100110011001100110011001100110011010

同理, 0.2 表示的完整表示是：

> 0 01111111100 1001100110011001100110011001100110011001100110011010

所以当 0.1 存储下来的时候，已经发生精度丢失，当我们使用浮点数进行运算的时候，使用的其实是精度丢失后的数

# 浮点数运算
浮点数运算，一般由以下五个步骤完成：对阶、尾数运算、规格化、舍入处理、溢出判断。  

经过一系列运算后，最终 0.1 + 0.2 得到的 64 位字节是  
> 0 01111111101 0011001100110011001100110011001100110011001100110100
转换成 10 进制就是`0.30000000000000004440892098500626`

所以，因为两次存储时的精度丢失加上运算时的精度丢失，最终导致`0.1 + 0.2 !== 0.3`

# 参考
- [JavaScript 深入之浮点数精度](https://github.com/mqyqingfeng/Blog/issues/155)
- [IEEE-754标准与浮点数运算](https://blog.csdn.net/m0_37972557/article/details/84594879)